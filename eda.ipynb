{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f64f8fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV: data\\ai_improved.csv  — rows: 212, columns: 30\n",
      "\n",
      "Columns (name : dtype — unique values / % null):\n",
      " - credit_payment : float64 — unique=1, pct_null=2.36%\n",
      " - store_name : object — unique=19, pct_null=1.42%\n",
      " - taxpayer_name : object — unique=16, pct_null=1.42%\n",
      " - store_code : object — unique=15, pct_null=1.42%\n",
      " - tax_id : float64 — unique=13, pct_null=1.42%\n",
      " - store_address : object — unique=29, pct_null=1.42%\n",
      " - item_name : object — unique=152, pct_null=1.42%\n",
      " - cashier_name : object — unique=30, pct_null=1.42%\n",
      " - date : object — unique=51, pct_null=1.42%\n",
      " - time : object — unique=59, pct_null=1.42%\n",
      " - unit_price : float64 — unique=89, pct_null=1.42%\n",
      " - quantity : float64 — unique=19, pct_null=1.42%\n",
      " - line_total : float64 — unique=98, pct_null=1.42%\n",
      " - receipt_number : object — unique=59, pct_null=1.42%\n",
      " - fiscal_registration : float64 — unique=24, pct_null=1.42%\n",
      " - subtotal : float64 — unique=58, pct_null=1.42%\n",
      " - total_tax : float64 — unique=51, pct_null=1.42%\n",
      " - vat_18_percent : float64 — unique=51, pct_null=1.42%\n",
      " - fiscal_id : object — unique=59, pct_null=1.42%\n",
      " - cash_register_serial : object — unique=24, pct_null=1.42%\n",
      " - cash_register_model : object — unique=11, pct_null=1.42%\n",
      " - queue_number : float64 — unique=57, pct_null=1.42%\n",
      " - refund_date : object — unique=48, pct_null=1.42%\n",
      " - refund_amount : float64 — unique=28, pct_null=1.42%\n",
      " - refund_time : object — unique=58, pct_null=1.42%\n",
      " - advance_payment : float64 — unique=2, pct_null=0.94%\n",
      " - cash_payment : float64 — unique=2, pct_null=0.94%\n",
      " - bonus_payment : float64 — unique=1, pct_null=0.94%\n",
      " - cashless_payment : float64 — unique=56, pct_null=0.94%\n",
      " - filename : object — unique=62, pct_null=0.00%\n",
      "\n",
      "Data preview (first 6 rows):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>store_name</th>\n",
       "      <th>store_address</th>\n",
       "      <th>store_code</th>\n",
       "      <th>taxpayer_name</th>\n",
       "      <th>tax_id</th>\n",
       "      <th>receipt_number</th>\n",
       "      <th>cashier_name</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>advance_payment</th>\n",
       "      <th>credit_payment</th>\n",
       "      <th>queue_number</th>\n",
       "      <th>cash_register_model</th>\n",
       "      <th>cash_register_serial</th>\n",
       "      <th>fiscal_id</th>\n",
       "      <th>fiscal_registration</th>\n",
       "      <th>refund_amount</th>\n",
       "      <th>refund_date</th>\n",
       "      <th>refund_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9FjTM9ngrpCu.jpeg</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...</td>\n",
       "      <td>1702278621-14001</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>1.702279e+09</td>\n",
       "      <td>16538</td>\n",
       "      <td>Hasanova Gulnar</td>\n",
       "      <td>15.01.2024</td>\n",
       "      <td>08:30:11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NCR RealPOS XR7 (7703) KLR M-POS 1.05.90</td>\n",
       "      <td>0000035727</td>\n",
       "      <td>9FİTMƏngrpCu</td>\n",
       "      <td>163236.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>15.02.2024</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9FjTM9ngrpCu.jpeg</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...</td>\n",
       "      <td>1702278621-14001</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>1.702279e+09</td>\n",
       "      <td>16538</td>\n",
       "      <td>Hasanova Gulnar</td>\n",
       "      <td>15.01.2024</td>\n",
       "      <td>08:30:11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NCR RealPOS XR7 (7703) KLR M-POS 1.05.90</td>\n",
       "      <td>0000035727</td>\n",
       "      <td>9FİTMƏngrpCu</td>\n",
       "      <td>163236.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>15.02.2024</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9FjTM9ngrpCu.jpeg</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...</td>\n",
       "      <td>1702278621-14001</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>1.702279e+09</td>\n",
       "      <td>16538</td>\n",
       "      <td>Hasanova Gulnar</td>\n",
       "      <td>15.01.2024</td>\n",
       "      <td>08:30:11</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NCR RealPOS XR7 (7703) KLR M-POS 1.05.90</td>\n",
       "      <td>0000035727</td>\n",
       "      <td>9FİTMƏngrpCu</td>\n",
       "      <td>163236.0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>15.02.2024</td>\n",
       "      <td>06:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zbRTryY1MVkR.jpeg</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...</td>\n",
       "      <td>1702278621-14001</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>1.702279e+09</td>\n",
       "      <td>786717</td>\n",
       "      <td>Quliyeva Gulnar</td>\n",
       "      <td>09.06.2023</td>\n",
       "      <td>08:20:47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>IBM POS 700</td>\n",
       "      <td>41ARR40</td>\n",
       "      <td>zbRTiyYIMVKR</td>\n",
       "      <td>21835.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>09.07.2023</td>\n",
       "      <td>12:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zbRTryY1MVkR.jpeg</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...</td>\n",
       "      <td>1702278621-14001</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>1.702279e+09</td>\n",
       "      <td>786717</td>\n",
       "      <td>Quliyeva Gulnar</td>\n",
       "      <td>09.06.2023</td>\n",
       "      <td>08:20:47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>IBM POS 700</td>\n",
       "      <td>41ARR40</td>\n",
       "      <td>zbRTiyYIMVKR</td>\n",
       "      <td>21835.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>09.07.2023</td>\n",
       "      <td>12:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>zbRTryY1MVkR.jpeg</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...</td>\n",
       "      <td>1702278621-14001</td>\n",
       "      <td>PROMART Məhdud Məsuliyyətli Cəmiyyəti</td>\n",
       "      <td>1.702279e+09</td>\n",
       "      <td>786717</td>\n",
       "      <td>Quliyeva Gulnar</td>\n",
       "      <td>09.06.2023</td>\n",
       "      <td>08:20:47</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>IBM POS 700</td>\n",
       "      <td>41ARR40</td>\n",
       "      <td>zbRTiyYIMVKR</td>\n",
       "      <td>21835.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>09.07.2023</td>\n",
       "      <td>12:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            filename                             store_name  \\\n",
       "0  9FjTM9ngrpCu.jpeg  PROMART Məhdud Məsuliyyətli Cəmiyyəti   \n",
       "1  9FjTM9ngrpCu.jpeg  PROMART Məhdud Məsuliyyətli Cəmiyyəti   \n",
       "2  9FjTM9ngrpCu.jpeg  PROMART Məhdud Məsuliyyətli Cəmiyyəti   \n",
       "3  zbRTryY1MVkR.jpeg  PROMART Məhdud Məsuliyyətli Cəmiyyəti   \n",
       "4  zbRTryY1MVkR.jpeg  PROMART Məhdud Məsuliyyətli Cəmiyyəti   \n",
       "5  zbRTryY1MVkR.jpeg  PROMART Məhdud Məsuliyyətli Cəmiyyəti   \n",
       "\n",
       "                                       store_address        store_code  \\\n",
       "0  AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...  1702278621-14001   \n",
       "1  AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...  1702278621-14001   \n",
       "2  AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...  1702278621-14001   \n",
       "3  AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...  1702278621-14001   \n",
       "4  AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...  1702278621-14001   \n",
       "5  AZ1007 BAKI ŞƏHƏRİ, NƏRİMANOV RAYONU, SÜLEYMAN...  1702278621-14001   \n",
       "\n",
       "                           taxpayer_name        tax_id receipt_number  \\\n",
       "0  PROMART Məhdud Məsuliyyətli Cəmiyyəti  1.702279e+09          16538   \n",
       "1  PROMART Məhdud Məsuliyyətli Cəmiyyəti  1.702279e+09          16538   \n",
       "2  PROMART Məhdud Məsuliyyətli Cəmiyyəti  1.702279e+09          16538   \n",
       "3  PROMART Məhdud Məsuliyyətli Cəmiyyəti  1.702279e+09         786717   \n",
       "4  PROMART Məhdud Məsuliyyətli Cəmiyyəti  1.702279e+09         786717   \n",
       "5  PROMART Məhdud Məsuliyyətli Cəmiyyəti  1.702279e+09         786717   \n",
       "\n",
       "      cashier_name        date      time  ... advance_payment  credit_payment  \\\n",
       "0  Hasanova Gulnar  15.01.2024  08:30:11  ...             0.0             0.0   \n",
       "1  Hasanova Gulnar  15.01.2024  08:30:11  ...             0.0             0.0   \n",
       "2  Hasanova Gulnar  15.01.2024  08:30:11  ...             0.0             0.0   \n",
       "3  Quliyeva Gulnar  09.06.2023  08:20:47  ...             0.0             0.0   \n",
       "4  Quliyeva Gulnar  09.06.2023  08:20:47  ...             0.0             0.0   \n",
       "5  Quliyeva Gulnar  09.06.2023  08:20:47  ...             0.0             0.0   \n",
       "\n",
       "   queue_number                       cash_register_model  \\\n",
       "0         319.0  NCR RealPOS XR7 (7703) KLR M-POS 1.05.90   \n",
       "1         319.0  NCR RealPOS XR7 (7703) KLR M-POS 1.05.90   \n",
       "2         319.0  NCR RealPOS XR7 (7703) KLR M-POS 1.05.90   \n",
       "3          90.0                               IBM POS 700   \n",
       "4          90.0                               IBM POS 700   \n",
       "5          90.0                               IBM POS 700   \n",
       "\n",
       "   cash_register_serial     fiscal_id  fiscal_registration  refund_amount  \\\n",
       "0            0000035727  9FİTMƏngrpCu             163236.0           0.17   \n",
       "1            0000035727  9FİTMƏngrpCu             163236.0           0.17   \n",
       "2            0000035727  9FİTMƏngrpCu             163236.0           0.17   \n",
       "3               41ARR40  zbRTiyYIMVKR              21835.0           0.09   \n",
       "4               41ARR40  zbRTiyYIMVKR              21835.0           0.09   \n",
       "5               41ARR40  zbRTiyYIMVKR              21835.0           0.09   \n",
       "\n",
       "   refund_date  refund_time  \n",
       "0   15.02.2024        06:01  \n",
       "1   15.02.2024        06:01  \n",
       "2   15.02.2024        06:01  \n",
       "3   09.07.2023        12:03  \n",
       "4   09.07.2023        12:03  \n",
       "5   09.07.2023        12:03  \n",
       "\n",
       "[6 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote: data\\charts\\column_summary.csv and data\\charts\\sample_head_200.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Load CSV and inspect actual columns & samples (data-aware)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# CONFIG - change only if your dataset is elsewhere\n",
    "CSV_PATH = Path(\"data/ai_improved.csv\")\n",
    "CHARTS_DIR = Path(\"data/charts\")\n",
    "CHARTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not CSV_PATH.exists():\n",
    "    raise FileNotFoundError(f\"CSV not found at {CSV_PATH}. Place dataset there or update CSV_PATH.\")\n",
    "\n",
    "# Load with low_memory=False to avoid dtype churning\n",
    "df = pd.read_csv(CSV_PATH, low_memory=False)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "print(f\"Loaded CSV: {CSV_PATH}  — rows: {len(df):,}, columns: {len(df.columns):,}\\n\")\n",
    "\n",
    "# Basic column summary\n",
    "col_summary = []\n",
    "for c in df.columns:\n",
    "    non_null = df[c].notna().sum()\n",
    "    pct_null = 1 - (non_null / max(1, len(df))\n",
    "    )\n",
    "    col_summary.append({\n",
    "        \"column\": c,\n",
    "        \"dtype\": str(df[c].dtype),\n",
    "        \"n_unique\": df[c].nunique(dropna=True),\n",
    "        \"pct_null\": pct_null\n",
    "    })\n",
    "\n",
    "col_summary_df = pd.DataFrame(col_summary).sort_values(by=\"pct_null\", ascending=False)\n",
    "col_summary_df.to_csv(CHARTS_DIR / \"column_summary.csv\", index=False)\n",
    "\n",
    "# Print columns w/ types and top unique counts\n",
    "print(\"Columns (name : dtype — unique values / % null):\")\n",
    "for _, row in col_summary_df.iterrows():\n",
    "    print(f\" - {row['column']} : {row['dtype']} — unique={int(row['n_unique'])}, pct_null={row['pct_null']:.2%}\")\n",
    "\n",
    "# Show first 6 rows for visual confirmation\n",
    "print(\"\\nData preview (first 6 rows):\")\n",
    "display(df.head(6))\n",
    "\n",
    "# Save a small sample file for quick manual check\n",
    "df.head(200).to_csv(CHARTS_DIR / \"sample_head_200.csv\", index=False)\n",
    "\n",
    "# Persist a JSON of columns/types for downstream deterministic selection\n",
    "with open(CHARTS_DIR / \"columns_meta.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump({\"n_rows\": len(df), \"columns\": {c: str(df[c].dtype) for c in df.columns}}, f, indent=2)\n",
    "\n",
    "print(f\"\\nWrote: {CHARTS_DIR / 'column_summary.csv'} and {CHARTS_DIR / 'sample_head_200.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8c56b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Date candidates (column, fraction parsed):\n",
      " - date : 98.58%\n",
      " - time : 98.58%\n",
      " - refund_date : 98.58%\n",
      " - refund_time : 98.58%\n",
      " - __parsed_date : 98.58%\n",
      " - __year_month : 98.58%\n",
      "\n",
      "Numeric candidates (column, fraction non-null or coercible):\n",
      " - cashless_payment : 99.06% (native_numeric)\n",
      " - cash_payment : 99.06% (native_numeric)\n",
      " - bonus_payment : 99.06% (native_numeric)\n",
      " - advance_payment : 99.06% (native_numeric)\n",
      " - __amount_clean : 99.06% (native_numeric)\n",
      " - tax_id : 98.58% (native_numeric)\n",
      " - quantity : 98.58% (native_numeric)\n",
      " - unit_price : 98.58% (native_numeric)\n",
      " - line_total : 98.58% (native_numeric)\n",
      " - subtotal : 98.58% (native_numeric)\n",
      " - vat_18_percent : 98.58% (native_numeric)\n",
      " - total_tax : 98.58% (native_numeric)\n",
      "\n",
      "Selected primary_date_col  -> date\n",
      "Selected primary_amount_col -> cashless_payment\n",
      "\n",
      "Parsing 'date' -> parsed fraction: 98.58%\n",
      "Created '__parsed_date' with 209 non-null values.\n",
      "\n",
      "Created '__amount_clean' from 'cashless_payment' with 210 non-null numeric values.\n",
      "\n",
      "Created derived __year_month and __day_of_week columns (based on parsed dates).\n",
      "\n",
      "Saved selected_columns.json to charts directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\2009323800.py:17: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Robustly detect best date and numeric columns (with explicit override options)\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "CHARTS_DIR = Path(\"data/charts\")\n",
    "\n",
    "# ---------- USER OVERRIDES ----------\n",
    "# If you know the exact column names, fill them here (exact string match)\n",
    "FORCE_DATE_COL = None      # e.g. \"receipt_dt\" or \"transaction_date\"\n",
    "FORCE_AMOUNT_COL = None    # e.g. \"total_amount\" or \"amount\"\n",
    "\n",
    "# ---------- Helper functions ----------\n",
    "def date_parse_fraction(series):\n",
    "    # attempt parsing after casting to string (safe)\n",
    "    parsed = pd.to_datetime(series.astype(str), errors=\"coerce\", dayfirst=False)\n",
    "    return parsed.notna().sum() / max(1, len(parsed)), parsed\n",
    "\n",
    "def numeric_parse_fraction(series):\n",
    "    coerced = pd.to_numeric(series.astype(str).str.replace(\",\",\"\").str.strip(), errors=\"coerce\")\n",
    "    return coerced.notna().sum() / max(1, len(coerced)), coerced\n",
    "\n",
    "# ---------- Candidate scoring ----------\n",
    "date_candidates = []\n",
    "numeric_candidates = []\n",
    "\n",
    "for c in df.columns:\n",
    "    # date check\n",
    "    frac_date, parsed = date_parse_fraction(df[c])\n",
    "    if frac_date >= 0.20:   # 20% threshold -> show as candidate (we will prefer higher)\n",
    "        date_candidates.append((c, float(frac_date)))\n",
    "    # numeric check (explicit numeric dtype or coercible)\n",
    "    if pd.api.types.is_numeric_dtype(df[c]):\n",
    "        frac_num = df[c].notna().mean()\n",
    "        numeric_candidates.append((c, float(frac_num), True))\n",
    "    else:\n",
    "        frac_num, coerced = numeric_parse_fraction(df[c])\n",
    "        if frac_num >= 0.60:  # 60% coercion threshold\n",
    "            # Create a coerced column name for safe use later\n",
    "            newname = f\"{c}_num_inferred\"\n",
    "            # only create if not already created\n",
    "            if newname not in df.columns:\n",
    "                df[newname] = coerced\n",
    "            numeric_candidates.append((newname, float(frac_num), False))\n",
    "\n",
    "# Sort by fraction descending\n",
    "date_candidates = sorted(date_candidates, key=lambda x: -x[1])\n",
    "numeric_candidates = sorted(numeric_candidates, key=lambda x: -x[1])\n",
    "\n",
    "# Report to user exactly what was found\n",
    "print(\"\\nDate candidates (column, fraction parsed):\")\n",
    "for c, frac in date_candidates[:8]:\n",
    "    print(f\" - {c} : {frac:.2%}\")\n",
    "\n",
    "print(\"\\nNumeric candidates (column, fraction non-null or coercible):\")\n",
    "for c, frac, is_native in numeric_candidates[:12]:\n",
    "    tag = \"native_numeric\" if is_native else \"inferred_numeric\"\n",
    "    print(f\" - {c} : {frac:.2%} ({tag})\")\n",
    "\n",
    "# ---------- Select primary columns (apply override if given) ----------\n",
    "if FORCE_DATE_COL:\n",
    "    if FORCE_DATE_COL not in df.columns:\n",
    "        raise KeyError(f\"FORCE_DATE_COL '{FORCE_DATE_COL}' not found in dataframe columns.\")\n",
    "    primary_date_col = FORCE_DATE_COL\n",
    "else:\n",
    "    primary_date_col = date_candidates[0][0] if date_candidates else None\n",
    "\n",
    "if FORCE_AMOUNT_COL:\n",
    "    if FORCE_AMOUNT_COL not in df.columns:\n",
    "        raise KeyError(f\"FORCE_AMOUNT_COL '{FORCE_AMOUNT_COL}' not found in dataframe columns.\")\n",
    "    primary_amount_col = FORCE_AMOUNT_COL\n",
    "else:\n",
    "    primary_amount_col = numeric_candidates[0][0] if numeric_candidates else None\n",
    "\n",
    "print(\"\\nSelected primary_date_col  ->\", primary_date_col)\n",
    "print(\"Selected primary_amount_col ->\", primary_amount_col)\n",
    "\n",
    "# ---------- Validate & create sanitized helper columns ----------\n",
    "# Create parsed date only if it parses sensibly\n",
    "parsed_date_col = \"__parsed_date\"\n",
    "if primary_date_col:\n",
    "    frac_date, parsed_series = date_parse_fraction(df[primary_date_col])\n",
    "    print(f\"\\nParsing '{primary_date_col}' -> parsed fraction: {frac_date:.2%}\")\n",
    "    if frac_date >= 0.05:   # even small parsing fraction we keep parsed_date but warn\n",
    "        df[parsed_date_col] = parsed_series\n",
    "        parsed_non_null = df[parsed_date_col].notna().sum()\n",
    "        print(f\"Created '{parsed_date_col}' with {parsed_non_null} non-null values.\")\n",
    "    else:\n",
    "        print(f\"Warning: column '{primary_date_col}' parsed only {frac_date:.2%}. Parsed date column will not be created.\")\n",
    "        parsed_date_col = None\n",
    "else:\n",
    "    parsed_date_col = None\n",
    "    print(\"\\nNo date column selected.\")\n",
    "\n",
    "# Create cleaned amount numeric column\n",
    "clean_amount_col = \"__amount_clean\"\n",
    "if primary_amount_col:\n",
    "    # attempt coercion to numeric (if already numeric, this is fast)\n",
    "    try:\n",
    "        amount_series = pd.to_numeric(df[primary_amount_col].astype(str).str.replace(\",\",\"\").str.strip(), errors=\"coerce\")\n",
    "    except Exception:\n",
    "        amount_series = pd.to_numeric(df[primary_amount_col], errors=\"coerce\")\n",
    "    df[clean_amount_col] = amount_series\n",
    "    n_amt = df[clean_amount_col].notna().sum()\n",
    "    print(f\"\\nCreated '{clean_amount_col}' from '{primary_amount_col}' with {n_amt} non-null numeric values.\")\n",
    "else:\n",
    "    print(\"\\nNo amount column selected; cannot create a cleaned numeric amount.\")\n",
    "\n",
    "# Derived helpers if parsed_date_col exists\n",
    "if parsed_date_col:\n",
    "    df[\"__year_month\"] = df[parsed_date_col].dt.to_period(\"M\").astype(str)\n",
    "    df[\"__day_of_week\"] = df[parsed_date_col].dt.day_name()\n",
    "    print(\"\\nCreated derived __year_month and __day_of_week columns (based on parsed dates).\")\n",
    "else:\n",
    "    print(\"\\nSkipped creating __year_month and __day_of_week because parsed date column is missing.\")\n",
    "\n",
    "# Persist the selection (deterministic)\n",
    "selection = {\n",
    "    \"primary_date_col\": primary_date_col,\n",
    "    \"primary_amount_col\": primary_amount_col,\n",
    "    \"parsed_date_col\": parsed_date_col,\n",
    "    \"clean_amount_col\": clean_amount_col\n",
    "}\n",
    "with open(CHARTS_DIR / \"selected_columns.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(selection, f, indent=2)\n",
    "print(\"\\nSaved selected_columns.json to charts directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34da18db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iasamadov\\AppData\\Local\\Temp\\ipykernel_22988\\1195533120.py:101: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  ax.boxplot(groups, labels=labels, vert=True, sym='o')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chart 6 skipped: score-like numeric not found or amount missing.\n",
      "Chart 12 skipped: status-like column not found or amount missing.\n",
      "\n",
      "Done. Charts saved to: data\\charts\n",
      "ZIP archive at: \\mnt\\data\\charts.zip\n",
      "\n",
      "Files created (manifest):\n",
      "Generated charts and captions:\n",
      "\n",
      "01_monthly_total___amount_clean.png : Monthly total — reveals seasonality and funding needs.\n",
      "02_monthly_count.png : Monthly record count — staffing & workload forecasting.\n",
      "03_distribution___amount_clean.png : Distribution — shows skew/outliers to guide thresholds and pricing.\n",
      "04_boxplot___amount_clean_by_store_name.png : Boxplot by store_name — compare dispersion and outliers across groups.\n",
      "05_top10_store_name_by___amount_clean.png : Top customers by total — prioritize retention & upsell.\n",
      "07_avg___amount_clean_by_store_name.png : Average by branch — resource & incentive planning.\n",
      "08_avg_by_dayofweek.png : Weekday patterns — staffing & campaign timing.\n",
      "09_ecdf___amount_clean.png : ECDF — percentile thresholds for business rules.\n",
      "10_correlation_matrix.png : Correlation matrix — feature engineering guidance.\n",
      "11_missingness_top20.png : Missingness profile — prioritize data-quality fixes.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Generate charts using the deterministic selection from Cell 2.\n",
    "# This cell will skip any chart where required columns / amounts / rows are missing.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import zipfile\n",
    "import math\n",
    "\n",
    "# Load selection\n",
    "sel_path = CHARTS_DIR / \"selected_columns.json\"\n",
    "if not sel_path.exists():\n",
    "    raise FileNotFoundError(\"selected_columns.json not found. Run Cell 2 first.\")\n",
    "with open(sel_path, \"r\", encoding=\"utf8\") as f:\n",
    "    selection = json.load(f)\n",
    "\n",
    "parsed_date_col = selection.get(\"parsed_date_col\")\n",
    "clean_amount_col = selection.get(\"clean_amount_col\")\n",
    "\n",
    "saved_metadata = []  # list of (filename, caption)\n",
    "\n",
    "def save_fig(fig, fname, caption):\n",
    "    path = CHARTS_DIR / fname\n",
    "    fig.savefig(path, bbox_inches=\"tight\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    saved_metadata.append((path.name, caption))\n",
    "    return path\n",
    "\n",
    "def safe_as_float(series):\n",
    "    return pd.to_numeric(series, errors=\"coerce\").astype(float)\n",
    "\n",
    "# Chart 1: Monthly total (requires __year_month and clean amount)\n",
    "if \"__year_month\" in df.columns and clean_amount_col in df.columns:\n",
    "    tmp = df.dropna(subset=[\"__year_month\", clean_amount_col])\n",
    "    if not tmp.empty:\n",
    "        monthly = tmp.groupby(\"__year_month\")[clean_amount_col].sum().sort_index()\n",
    "        fig, ax = plt.subplots(figsize=(10,4))\n",
    "        ax.plot(monthly.index, monthly.values, marker='o')\n",
    "        ax.set_title(f\"Monthly total of {clean_amount_col}\")\n",
    "        ax.set_xlabel(\"Year-Month\")\n",
    "        ax.set_ylabel(\"Total\")\n",
    "        plt.xticks(rotation=45)\n",
    "        save_fig(fig, f\"01_monthly_total_{clean_amount_col}.png\",\n",
    "                 \"Monthly total — reveals seasonality and funding needs.\")\n",
    "    else:\n",
    "        print(\"Chart 1 skipped: after dropna no rows available for monthly totals.\")\n",
    "else:\n",
    "    print(\"Chart 1 skipped: missing __year_month or amount column.\")\n",
    "\n",
    "# Chart 2: Monthly count (if __year_month exists)\n",
    "if \"__year_month\" in df.columns:\n",
    "    monthly_cnt = df.groupby(\"__year_month\").size().sort_index()\n",
    "    if not monthly_cnt.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10,4))\n",
    "        ax.bar(monthly_cnt.index, monthly_cnt.values)\n",
    "        ax.set_title(\"Monthly record count\")\n",
    "        ax.set_xlabel(\"Year-Month\")\n",
    "        ax.set_ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        save_fig(fig, \"02_monthly_count.png\", \"Monthly record count — staffing & workload forecasting.\")\n",
    "    else:\n",
    "        print(\"Chart 2 skipped: monthly count computation produced empty result.\")\n",
    "else:\n",
    "    print(\"Chart 2 skipped: __year_month not present.\")\n",
    "\n",
    "# Chart 3: Distribution (histogram) of amount\n",
    "if clean_amount_col in df.columns:\n",
    "    arr = df[clean_amount_col].dropna().astype(float)\n",
    "    if len(arr) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        ax.hist(arr, bins=40)\n",
    "        ax.set_title(f\"Distribution of {clean_amount_col}\")\n",
    "        ax.set_xlabel(clean_amount_col)\n",
    "        ax.set_ylabel(\"Frequency\")\n",
    "        save_fig(fig, f\"03_distribution_{clean_amount_col}.png\", \n",
    "                 \"Distribution — shows skew/outliers to guide thresholds and pricing.\")\n",
    "    else:\n",
    "        print(\"Chart 3 skipped: amount column exists but has no non-null values.\")\n",
    "else:\n",
    "    print(\"Chart 3 skipped: cleaned amount column missing.\")\n",
    "\n",
    "# Chart 4: Boxplot by a low-cardinality categorical column\n",
    "# find a categorical column automatically (exclude parsed/clean columns)\n",
    "cat_col = None\n",
    "for c in df.columns:\n",
    "    if c in [parsed_date_col, clean_amount_col, \"__year_month\", \"__day_of_week\"]:\n",
    "        continue\n",
    "    if not pd.api.types.is_numeric_dtype(df[c]):\n",
    "        nu = df[c].nunique(dropna=True)\n",
    "        if 2 <= nu <= 30:\n",
    "            cat_col = c\n",
    "            break\n",
    "\n",
    "if cat_col and clean_amount_col in df.columns:\n",
    "    groups_index = df[cat_col].value_counts().index[:12]\n",
    "    groups = [df.loc[df[cat_col] == v, clean_amount_col].dropna().astype(float) for v in groups_index]\n",
    "    groups = [g for g in groups if len(g) > 0]\n",
    "    labels = [str(v) for v in groups_index[:len(groups)]]\n",
    "    if groups:\n",
    "        fig, ax = plt.subplots(figsize=(12,5))\n",
    "        ax.boxplot(groups, labels=labels, vert=True, sym='o')\n",
    "        ax.set_title(f\"Boxplot of {clean_amount_col} by {cat_col}\")\n",
    "        plt.xticks(rotation=45)\n",
    "        save_fig(fig, f\"04_boxplot_{clean_amount_col}_by_{cat_col}.png\",\n",
    "                 f\"Boxplot by {cat_col} — compare dispersion and outliers across groups.\")\n",
    "    else:\n",
    "        print(\"Chart 4 skipped: no groups with non-empty amounts.\")\n",
    "else:\n",
    "    print(\"Chart 4 skipped: no low-cardinality categorical found or amount missing.\")\n",
    "\n",
    "# Chart 5: Top 10 customers by total amount (if client-like column found)\n",
    "client_candidate = None\n",
    "client_keywords = [\"client\",\"cust\",\"account\",\"acct\",\"card\",\"customer\",\"shop\",\"store\",\"merchant\",\"t_lclient\"]\n",
    "for kw in client_keywords:\n",
    "    for c in df.columns:\n",
    "        if kw.lower() in c.lower():\n",
    "            client_candidate = c\n",
    "            break\n",
    "    if client_candidate:\n",
    "        break\n",
    "\n",
    "if client_candidate and clean_amount_col in df.columns:\n",
    "    top_clients = df.groupby(client_candidate)[clean_amount_col].sum().sort_values(ascending=False).head(10)\n",
    "    if not top_clients.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        ax.bar(top_clients.index.astype(str), top_clients.values)\n",
    "        plt.xticks(rotation=45)\n",
    "        ax.set_title(f\"Top 10 {client_candidate} by total {clean_amount_col}\")\n",
    "        save_fig(fig, f\"05_top10_{client_candidate}_by_{clean_amount_col}.png\",\n",
    "                 \"Top customers by total — prioritize retention & upsell.\")\n",
    "    else:\n",
    "        print(\"Chart 5 skipped: aggregation returned empty.\")\n",
    "else:\n",
    "    print(\"Chart 5 skipped: no client-like column or amount missing.\")\n",
    "\n",
    "# Chart 6: Scatter vs score-like numeric column\n",
    "score_candidate = None\n",
    "score_keywords = [\"score\",\"credit_score\",\"rating\",\"risk\",\"risk_score\",\"scor\"]\n",
    "for kw in score_keywords:\n",
    "    for c in df.columns:\n",
    "        if kw.lower() in c.lower() and pd.api.types.is_numeric_dtype(df[c]):\n",
    "            score_candidate = c\n",
    "            break\n",
    "    if score_candidate:\n",
    "        break\n",
    "\n",
    "if score_candidate and clean_amount_col in df.columns:\n",
    "    sub = df.dropna(subset=[score_candidate, clean_amount_col])\n",
    "    if len(sub) > 10:\n",
    "        fig, ax = plt.subplots(figsize=(7,5))\n",
    "        ax.scatter(sub[score_candidate].astype(float), sub[clean_amount_col].astype(float), s=10)\n",
    "        ax.set_title(f\"{clean_amount_col} vs {score_candidate}\")\n",
    "        save_fig(fig, f\"06_scatter_{clean_amount_col}_vs_{score_candidate}.png\",\n",
    "                 \"Scatter — validate risk-to-price relationship.\")\n",
    "    else:\n",
    "        print(\"Chart 6 skipped: not enough points with both score and amount.\")\n",
    "else:\n",
    "    print(\"Chart 6 skipped: score-like numeric not found or amount missing.\")\n",
    "\n",
    "# Chart 7: Average amount by branch-like dimension\n",
    "branch_candidate = None\n",
    "branch_keywords = [\"branch\",\"office\",\"dep\",\"department\",\"region\",\"store\",\"shop\",\"merchant\",\"t_branch\",\"t_kuratorcode\"]\n",
    "for kw in branch_keywords:\n",
    "    for c in df.columns:\n",
    "        if kw.lower() in c.lower():\n",
    "            branch_candidate = c\n",
    "            break\n",
    "    if branch_candidate:\n",
    "        break\n",
    "\n",
    "if branch_candidate and clean_amount_col in df.columns:\n",
    "    avg_branch = df.groupby(branch_candidate)[clean_amount_col].mean().sort_values(ascending=False).head(20)\n",
    "    if not avg_branch.empty:\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        ax.bar(avg_branch.index.astype(str), avg_branch.values)\n",
    "        plt.xticks(rotation=45)\n",
    "        ax.set_title(f\"Average {clean_amount_col} by {branch_candidate} (top 20)\")\n",
    "        save_fig(fig, f\"07_avg_{clean_amount_col}_by_{branch_candidate}.png\",\n",
    "                 \"Average by branch — resource & incentive planning.\")\n",
    "    else:\n",
    "        print(\"Chart 7 skipped: branch aggregation empty.\")\n",
    "else:\n",
    "    print(\"Chart 7 skipped: branch-like column not found or amount missing.\")\n",
    "\n",
    "# Chart 8: Average by day-of-week (if exists)\n",
    "if \"__day_of_week\" in df.columns and clean_amount_col in df.columns:\n",
    "    dow = df.dropna(subset=[\"__day_of_week\", clean_amount_col]).groupby(\"__day_of_week\")[clean_amount_col].mean()\n",
    "    order = [\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\",\"Saturday\",\"Sunday\"]\n",
    "    dow = dow.reindex(order).dropna()\n",
    "    if not dow.empty:\n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        ax.bar(dow.index, dow.values)\n",
    "        ax.set_title(f\"Average {clean_amount_col} by day of week\")\n",
    "        save_fig(fig, \"08_avg_by_dayofweek.png\", \"Weekday patterns — staffing & campaign timing.\")\n",
    "    else:\n",
    "        print(\"Chart 8 skipped: day-of-week aggregation empty.\")\n",
    "else:\n",
    "    print(\"Chart 8 skipped: __day_of_week or amount missing.\")\n",
    "\n",
    "# Chart 9: ECDF of amount (if amount present)\n",
    "if clean_amount_col in df.columns:\n",
    "    vals = np.sort(df[clean_amount_col].dropna().astype(float).values)\n",
    "    if len(vals) > 0:\n",
    "        p = np.arange(1, len(vals)+1) / len(vals)\n",
    "        fig, ax = plt.subplots(figsize=(8,5))\n",
    "        ax.plot(vals, p, marker='.', linestyle='none')\n",
    "        ax.set_title(f\"ECDF of {clean_amount_col}\")\n",
    "        save_fig(fig, f\"09_ecdf_{clean_amount_col}.png\", \"ECDF — percentile thresholds for business rules.\")\n",
    "    else:\n",
    "        print(\"Chart 9 skipped: amount column empty.\")\n",
    "else:\n",
    "    print(\"Chart 9 skipped: amount column missing.\")\n",
    "\n",
    "# Chart 10: Correlation matrix for numeric columns\n",
    "numcols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if len(numcols) >= 2:\n",
    "    corr = df[numcols].corr().fillna(0)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    im = ax.imshow(corr.values, interpolation='nearest', aspect='auto')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.set_xticks(range(len(numcols))); ax.set_xticklabels(numcols, rotation=90)\n",
    "    ax.set_yticks(range(len(numcols))); ax.set_yticklabels(numcols)\n",
    "    ax.set_title(\"Correlation matrix (numeric features)\")\n",
    "    save_fig(fig, \"10_correlation_matrix.png\", \"Correlation matrix — feature engineering guidance.\")\n",
    "else:\n",
    "    print(\"Chart 10 skipped: fewer than 2 numeric columns.\")\n",
    "\n",
    "# Chart 11: Missingness profile (top 20)\n",
    "pctnull = (df.isna().mean() * 100).sort_values(ascending=False).head(20)\n",
    "if len(pctnull) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    ax.bar(pctnull.index.astype(str), pctnull.values)\n",
    "    plt.xticks(rotation=45)\n",
    "    ax.set_title(\"Top 20 columns by % missing values\")\n",
    "    save_fig(fig, \"11_missingness_top20.png\", \"Missingness profile — prioritize data-quality fixes.\")\n",
    "else:\n",
    "    print(\"Chart 11 skipped: computed missingness empty.\")\n",
    "\n",
    "# Chart 12: Default / bad-outcome estimate by status-like column (heuristic)\n",
    "status_candidate = None\n",
    "status_keywords = [\"default\",\"is_default\",\"status\",\"outcome\",\"repaid\",\"closed\",\"paid\",\"payment_status\"]\n",
    "for kw in status_keywords:\n",
    "    for c in df.columns:\n",
    "        if kw.lower() in c.lower():\n",
    "            status_candidate = c\n",
    "            break\n",
    "    if status_candidate:\n",
    "        break\n",
    "\n",
    "if status_candidate and clean_amount_col in df.columns:\n",
    "    s = df[status_candidate].astype(str).str.lower()\n",
    "    is_bad = s.str.contains(\"default|fail|bad|nonpay|charged off|charge-off|charge off|1\", na=False)\n",
    "    temp = pd.DataFrame({status_candidate: s, \"is_bad\": is_bad, clean_amount_col: df[clean_amount_col]})\n",
    "    grouped = temp.groupby(status_candidate)[\"is_bad\"].mean().sort_values(ascending=False).head(20)\n",
    "    if len(grouped) > 0:\n",
    "        fig, ax = plt.subplots(figsize=(10,5))\n",
    "        ax.bar(grouped.index.astype(str), grouped.values)\n",
    "        plt.xticks(rotation=60)\n",
    "        ax.set_title(f\"Estimated bad/default rate by {status_candidate}\")\n",
    "        save_fig(fig, f\"12_default_rate_by_{status_candidate}.png\", \n",
    "                 \"Estimated default rate — segmentation for collections.\")\n",
    "    else:\n",
    "        print(\"Chart 12 skipped: grouping produced empty result.\")\n",
    "else:\n",
    "    print(\"Chart 12 skipped: status-like column not found or amount missing.\")\n",
    "\n",
    "# Finalize: write manifest and zip the charts\n",
    "manifest_path = CHARTS_DIR / \"manifest.txt\"\n",
    "with open(manifest_path, \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(\"Generated charts and captions:\\n\\n\")\n",
    "    for fname, caption in saved_metadata:\n",
    "        f.write(f\"{fname} : {caption}\\n\")\n",
    "\n",
    "zip_path = Path(\"/mnt/data/charts.zip\")\n",
    "with zipfile.ZipFile(zip_path, \"w\") as zf:\n",
    "    for p in CHARTS_DIR.glob(\"*\"):\n",
    "        zf.write(p, arcname=p.name)\n",
    "\n",
    "print(\"\\nDone. Charts saved to:\", CHARTS_DIR)\n",
    "print(\"ZIP archive at:\", zip_path)\n",
    "print(\"\\nFiles created (manifest):\")\n",
    "with open(manifest_path, \"r\", encoding=\"utf8\") as f:\n",
    "    print(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e31f5e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected columns summary (from Cell 2):\n",
      "{\n",
      "  \"primary_date_col\": \"date\",\n",
      "  \"primary_amount_col\": \"cashless_payment\",\n",
      "  \"parsed_date_col\": \"__parsed_date\",\n",
      "  \"clean_amount_col\": \"__amount_clean\"\n",
      "}\n",
      "\n",
      "Preview of selected columns (__parsed_date, __amount_clean):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__parsed_date</th>\n",
       "      <th>__amount_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-15</td>\n",
       "      <td>6.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-06-09</td>\n",
       "      <td>3.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>8.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>8.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>8.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-06-02</td>\n",
       "      <td>8.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2023-03-18</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   __parsed_date  __amount_clean\n",
       "0     2024-01-15            6.48\n",
       "1     2024-01-15            6.48\n",
       "2     2024-01-15            6.48\n",
       "3     2023-06-09            3.33\n",
       "4     2023-06-09            3.33\n",
       "5     2023-06-09            3.33\n",
       "6     2023-06-09            3.33\n",
       "7     2023-06-02            8.33\n",
       "8     2023-06-02            8.33\n",
       "9     2023-06-02            8.33\n",
       "10    2023-06-02            8.33\n",
       "11    2023-03-18           69.99"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Manifest file: data\\charts\\manifest.txt\n",
      "Charts ZIP: \\mnt\\data\\charts.zip\n",
      "\n",
      "If you want different charts or more domain-specific visualizations, set FORCE_DATE_COL/FORCE_AMOUNT_COL in Cell 2 to exact column names and re-run cells 2→3→4.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sel_path = CHARTS_DIR / \"selected_columns.json\"\n",
    "if sel_path.exists():\n",
    "    with open(sel_path, \"r\", encoding=\"utf8\") as f:\n",
    "        selection = json.load(f)\n",
    "else:\n",
    "    selection = {}\n",
    "\n",
    "print(\"Selected columns summary (from Cell 2):\")\n",
    "print(json.dumps(selection, indent=2))\n",
    "\n",
    "# Show a small preview of the parsed date and cleaned amount (if they exist)\n",
    "parsed_date_col = selection.get(\"parsed_date_col\")\n",
    "clean_amount_col = selection.get(\"clean_amount_col\")\n",
    "\n",
    "cols_to_show = [c for c in [parsed_date_col, clean_amount_col] if c and c in df.columns]\n",
    "if cols_to_show:\n",
    "    print(f\"\\nPreview of selected columns ({', '.join(cols_to_show)}):\")\n",
    "    display(df.loc[:, cols_to_show].head(12))\n",
    "else:\n",
    "    print(\"\\nNo selected parsed or cleaned columns found in dataframe. Re-run Cell 2 and check overrides.\")\n",
    "\n",
    "# Show manifest file path and charts zip\n",
    "manifest = CHARTS_DIR / \"manifest.txt\"\n",
    "zip_path = Path(\"/mnt/data/charts.zip\")\n",
    "print(\"\\nManifest file:\", manifest)\n",
    "print(\"Charts ZIP:\", zip_path)\n",
    "print(\"\\nIf you want different charts or more domain-specific visualizations, set FORCE_DATE_COL/FORCE_AMOUNT_COL in Cell 2 to exact column names and re-run cells 2→3→4.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5e45a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2827cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cfdc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95da623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062fb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
